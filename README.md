# A Large Language Model Made using Transformer Architecture #
This architecture is inspired from the *"Attention Is All You Need"* Paper , and resources by Andrej Karpathy. Many LLMs like GPT-2 are based on this architecture.

## Please Note : ##
- Due to contraints on Computation capacity , the model has been trained on a very limited text corpus.
- Model performance can be significantly increased by scaling it up , using more training data , using larger context sizes and scaling up other hyperparameters. (which requires significantly better hardware to train)

The train_1.ipynb file can be used to train the model on any small text corpus.
For illustration , the model is trained on a few texts , and the generated outputs are also provided in the repository.
